---
title: 'Machine Learning - Learning Lab 1 Independent Practice'
author: "Rachel Wong"
date: "`r format(Sys.Date(),'%B %e, %Y')`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

![](img/ML_P_Hx.jpg){width="30%"}

The final activity for each learning lab provides space to work with data and to reflect on how the concepts and techniques introduced in each lab might apply to your own research.

To earn a badge for each lab, you are required to respond to a set of prompts for two parts:

-   In Part I, you will reflect on your understanding of key concepts and begin to think about potential next steps for your own study.

-   In Part II, you will create a simple data product in R that demonstrates your ability to apply an analytic technique introduced in this learning lab.

### Part I: Reflect and Plan

Part A:

1.  How good was the machine learning model we developed in the guiding practice? Would you be comfortable using it to code more conversations? What if you read about someone using such a model as a reviewer of research? Please add your thoughts and reflections following the bullet point below.

-   I think a 0.87 prediction accuracy is fairly good. 
-   I would be comfortable using it to code more conversations. However, I guess the tricky part about the prediction     accuracy % values is that there is no specific benchmark for what's considered a 'good' or 'bad' model. I suppose     it comes down to the researchers, their justifications, and their reviewers.
-   How much of the data do researchers have to first manually code before running a model?

2.  How might the model be improved? Share any ideas you have at this time below:
-   Will adding more variables make a difference to the 'strength' of the model? 
-   We are only relying on a few variables. Are there other variables that might be more suitable? Is this part of        feature engineering?     
-   Larger samples? But how large is large enough?

Part B: Use the institutional library (e.g. [NCSU Library](https://www.lib.ncsu.edu/#articles)), [Google Scholar](https://scholar.google.com/) or search engine to locate a research article, presentation, or resource that applies machine learning to an educational context aligned with your research interests. More specifically, **locate a machine learning study that involve making predictions**.

1.  Provide an APA citation for your selected study.

    -   Gray, C. C., & Perkins, D. (2019). Utilizing early engagement and machine learning to predict student         outcomes. Computers & Education, 131, 22â€“32. https://doi.org/10.1016/j.compedu.2018.12.006


2.  What research questions were the authors of this study trying to address and why did they consider these questions important?

    -   The overarching goal of this study was to apply machine learning techniques to identify students who are most at-risk of dropping out of courses (or school altogether) as early as possible, so that appropriate tutor interventions can be provided. Specifically, the authors focused on student attendance/engagement to produce predictions of student outcomes.
    -   Early interventions are most beneficial for keeping students engaged and increasing student retention, especially for at-risk students. Early interventions focusing primarily on attendance or an academic skill have been associated with a 13% increase in retention

3.  What were the results of these analyses?

    -   From a set of experiments, the authos were able to extract a combination of classifier and measurements to aid in the early identification of students at-risk of dropping out. The model achieved an accuracy in excess of 97% in identifying at-risk students as early as week 3 in the Fall semester. 

### Part II: Data Product

For the data product, you are asked to dive into what it means for the model to be *predictively accurate*. Specifically, we'll explore some measures of just how predictively accurate the model we developed in the guided practice is.

We'll use a shortcut to cut to the chase -- interpreting the model. The code below loads the model we estimated in the guided practice -- in the form of the `final_fit`. This is necessary even if you currently have `final_fit` loaded in your environment/current R session, as you'll need to have everything generated by code in this document for it to successfully "knit".

```{r}
library(here)
library(readr)
library(tidymodels)

final_fit <- read_rds("out/ngsschat-final-fit.rds")

final_fit
```

Run the code below to calculate a \*confusion matrix\*

```{r}
final_fit$.predictions[[1]] %>% 
    conf_mat(.pred_class, code)
```

Please interpret the above confusion matrix using [these guidelines](https://towardsdatascience.com/understanding-confusion-matrix-a9ad42dcfd62) in terms of the true positive, true negative, false positive, and false negative rates. After each of the following (i.e., "True positive"), add both the *number* and percentage of observations. For instance, if there were 100 true positives out of a total of 400 data points, please write: 100 (25%).

**True positive**: 260 (34.3%)

**True negative**: 402 (53.0%)

**False positive**: 60 (7.9%)

**False negative**: 37 (4.9%)

You can read more about interpreting these [here](https://bradleyboehmke.github.io/HOML/process.html#classification-models) in terms of the specificity, sensitivity, precision, and recall, four statistics based on the information in the confusion matrix.

Return to your answer for Part 1A. Now, having examined the true and false positive and negative rates, how good do you think machine learning model we developed in the case study was? Write more specifically using the evidence you have from creating and interpreting the confusion matrix (above) after the following bullet point.

-   Accuracy 87.2% ((TP + TN)/total) ((260+402)/759)
-   Precision 81.3% (TP/(TP+FP)) (260/(260+60)))
-   Sensitivity 87.5% (TP/(TP + FN)) (260/(260+37))
-   Specificity 87.0% (TN/(TN+ FP)) (402/(402+60))

-   I think the machine learning model we developed in the case study is good. It has an 87.2% accuracy rate. In terms of precision, I'm guessing it could be better. Howveer, in terms of sensitivity and specificity, those percentages appear to be markers of a good model (though good seems subjective). 

### Knit & Submit

Congratulations, you've completed your Prediction badge! Complete the following steps to submit your work for review:

1.  Change the name of the `author:` in the [YAML header](https://monashdatafluency.github.io/r-rep-res/yaml-header.html) at the very top of this document to your name. As noted in [Reproducible Research in R](https://monashdatafluency.github.io/r-rep-res/index.html), The YAML header controls the style and feel for knitted document but doesn't actually display in the final output.

2.  Click the yarn icon above to "knit" your data product to a [HTML](https://bookdown.org/yihui/rmarkdown/html-document.html) file that will be saved in your R Project folder.

3.  Commit your changes in GitHub Desktop and push them to your online GitHub repository.

4.  Publish your HTML page the web using one of the following [publishing methods](https://rpubs.com/cathydatascience/518692):

    -   Publish on [RPubs](https://rpubs.com) by clicking the "Publish" button located in the Viewer Pane when you knit your document. Note, you will need to quickly create a RPubs account.

    -   Publishing on GitHub using either [GitHub Pages](https://pages.github.com) or the [HTML previewer](http://htmlpreview.github.io).

5.  Post a new discussion on GitHub to our [ML Badges forum](https://github.com/orgs/laser-institute/teams/machine-learning/discussions/2). In your post, include a link to your published web page and a short reflection highlighting one thing you learned from this lab and one thing you'd like to explore further.
